{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/gtechzilla/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gtechzilla/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import psycopg2\n",
    "import datetime\n",
    "import settings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "\n",
    "import re\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "from gensim.models import KeyedVectors\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import logging\n",
    "from numpy import random\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:10:00\n",
      "2020-05-25 02:18:08.279263\n",
      "                                                 text          created_at\n",
      "0   @hopefmlive @gichuru_george All I want is you ... 2020-05-25 02:18:25\n",
      "1   @C_K_Lomaria Am not around. Maliza, I will lau... 2020-05-25 02:19:01\n",
      "2                     Amen and amen \\n#barakazamilele 2020-05-25 02:20:31\n",
      "3   Bomboclaaaaaaaaaaat! Imma slit your throat wit... 2020-05-25 02:22:57\n",
      "4   Lovely and spacious 2 bedroom apartment to let... 2020-05-25 02:22:58\n",
      "5   @ahmedsalims Interesting I think people don't ... 2020-05-25 02:23:20\n",
      "6   Do not say, Why were the former days better th... 2020-05-25 02:24:18\n",
      "7   Early to bed early to rise. Good morning World... 2020-05-25 02:25:23\n",
      "8                  Good morning Liz \\n#barakazamilele 2020-05-25 02:25:38\n",
      "9   I say whoever cum first is loser in the game c... 2020-05-25 02:26:23\n",
      "10                   Good morning the prof himuselifu 2020-05-25 02:26:28\n"
     ]
    }
   ],
   "source": [
    "#connecting to db\n",
    "dbconn = psycopg2.connect(\"host=ec2-52-207-25-133.compute-1.amazonaws.com dbname=d8e9au4m77k9b1 user=twvlbubsgabvpj password=53cf31e1928ac9f0ec3ec5554a92bfa96ddb693b7bb3b31df2bbf3784cc66f6a\")\n",
    "\n",
    "#setting time interval from which to fetch the tweets from db\n",
    "time_now = datetime.datetime.utcnow()\n",
    "time_10mins_before = datetime.timedelta(hours=0,minutes=10)\n",
    "print(time_10mins_before)\n",
    "time_interval = time_now - time_10mins_before\n",
    "print(time_interval)\n",
    "#fetching tweets from db\n",
    "query = \"SELECT text, created_at FROM {} WHERE created_at >= '{}'\".format(settings.TABLE_NAME, time_interval)\n",
    "df = pd.read_sql(query,dbconn)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "#DATA CLEANING\n",
    "space_replace = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "bad_symbols = re.compile('[^0-9a-z #+_]')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "extensions = ['rt', 'RT']\n",
    "stopwords.extend(extensions)\n",
    "urls = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "            '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+' 'rt')\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "#wv = api.load('word2vec-google-news-300')\n",
    "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, limit=20000)\n",
    "word_vectors.init_sims(replace=True)\n",
    "\n",
    "#function to clean the twitter text\n",
    "def text_cleaning(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text #removing any html decoding\n",
    "    text = text.lower() #removing capitalization\n",
    "    text = space_replace.sub(' ', text)#replacing symbols with a space\n",
    "    text = bad_symbols.sub('', text) #deleting symbols from the text\n",
    "    text = ' '.join(word for word in text.split() if word not in stopwords) #removing stopwords\n",
    "    text = urls.sub('', text)#removing urls\n",
    "    return text\n",
    "\n",
    "\n",
    "#FEATURE ENGINEERING\n",
    "#word averaging\n",
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        \n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])\n",
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gtechzilla/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/home/gtechzilla/anaconda3/envs/nairobi_alive/lib/python3.7/site-packages/ipykernel_launcher.py:37: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "\n",
      "WARNING:root:cannot compute similarity with no input ['Amen', 'and', 'amen', 'barakazamilele']\n"
     ]
    }
   ],
   "source": [
    "df['text'] = df['text'].astype(str)\n",
    "nltk.download('punkt')\n",
    "data_tokenized = df.apply(lambda r: w2v_tokenize_text(r['text']), axis=1).values\n",
    "data_averaged = word_averaging_list(word_vectors,data_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gtechzilla/anaconda3/envs/nairobi_alive/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning:\n",
      "\n",
      "sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "\n",
      "/home/gtechzilla/anaconda3/envs/nairobi_alive/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning:\n",
      "\n",
      "Trying to unpickle estimator LogisticRegression from version 0.22.2.post1 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>staus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@hopefmlive @gichuru_george All I want is you ...</td>\n",
       "      <td>2020-05-25 02:18:25</td>\n",
       "      <td>Depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@C_K_Lomaria Am not around. Maliza, I will lau...</td>\n",
       "      <td>2020-05-25 02:19:01</td>\n",
       "      <td>Depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amen and amen \\n#barakazamilele</td>\n",
       "      <td>2020-05-25 02:20:31</td>\n",
       "      <td>Not Depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bomboclaaaaaaaaaaat! Imma slit your throat wit...</td>\n",
       "      <td>2020-05-25 02:22:57</td>\n",
       "      <td>Depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lovely and spacious 2 bedroom apartment to let...</td>\n",
       "      <td>2020-05-25 02:22:58</td>\n",
       "      <td>Not Depressed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text          created_at  \\\n",
       "0  @hopefmlive @gichuru_george All I want is you ... 2020-05-25 02:18:25   \n",
       "1  @C_K_Lomaria Am not around. Maliza, I will lau... 2020-05-25 02:19:01   \n",
       "2                    Amen and amen \\n#barakazamilele 2020-05-25 02:20:31   \n",
       "3  Bomboclaaaaaaaaaaat! Imma slit your throat wit... 2020-05-25 02:22:57   \n",
       "4  Lovely and spacious 2 bedroom apartment to let... 2020-05-25 02:22:58   \n",
       "\n",
       "           staus  \n",
       "0      Depressed  \n",
       "1      Depressed  \n",
       "2  Not Depressed  \n",
       "3      Depressed  \n",
       "4  Not Depressed  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib \n",
    "  \n",
    "# Load the model from the file \n",
    "depression_analyzer = joblib.load('testfile.sav')  \n",
    "  \n",
    "# Use the loaded model to make predictions \n",
    "df.text=df.text.astype(str)\n",
    "df['status'] = depression_analyzer.predict(data_averaged) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'status'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1bb3336ede35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#plottin our pie_chart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sex\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"total_bill\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nairobi_alive/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'status'"
     ]
    }
   ],
   "source": [
    "#plottin our pie_chart\n",
    "fig = px.bar(df.status(), x=\"sex\", y=\"total_bill\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
